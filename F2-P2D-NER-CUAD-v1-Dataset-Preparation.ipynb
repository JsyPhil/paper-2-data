{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paper 2 Data Workflow for Data Extraction - CUADv1 - Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, json, os, itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "from spacy.training import offsets_to_biluo_tags # requires spaCy 3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download transformer model for spaCy if required\n",
    "# !python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy validate # Ensure minimum v3.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. File handling - CUADv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MASTER_PATH = \"../CUAD-v1/\"\n",
    "JSONL_FILE = 'project_6_dataset.jsonl'\n",
    "JSONL_FILE_INS = 'project_7_dataset.jsonl'\n",
    "FEATURE_CLASS_LABELS = \"feature_class_labels.json\"\n",
    "DATA_FILE = 'cuad-v1-annotated.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Text Data Preprocessing - CUADv1 - Continued"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Doccano to tag the text file dataset:\n",
    " - Install doccano at the command line: pip install doccano\n",
    " - At the command line change the directory to this directory\n",
    " - run doccano at the command line by typing 'doccano'\n",
    " - Application will be running at http://0.0.0.0:8000/\n",
    " - Username is 'admin', passowrd is 'password'\n",
    " - Use ctrl-c to end application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare updated dataset for fine-tuning Transformers with HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSONL is a multi-line json file and requires lines=True parameter\n",
    "# Bring in both sets of annotations and concatenate vertically \n",
    "df1 = pd.read_json (JSONL_FILE, lines=True)\n",
    "df2 = pd.read_json (JSONL_FILE_INS, lines=True)\n",
    "\n",
    "df = pd.concat([df1, df2], axis=0)\n",
    "df = df1 # Use this line to exclude the additional manually checked data\n",
    "df = df.drop(['meta', 'annotation_approver', 'comments'], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the information and number of samples\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some samples were not annotated as they were not suitable samples.\n",
    "# Eliminate any samples which were not annotated.\n",
    "df_cut = df[df['labels'].map(lambda d: len(d)) > 0].copy()\n",
    "df_cut.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We tokenize each agreement prior to bringing into the transformer model\n",
    "# Create tokens using spaCy\n",
    "nlp = English()\n",
    "df_cut['tokens'] = df_cut['text'].apply(lambda x: nlp(x))\n",
    "df_cut.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check an example of the text indices and labels\n",
    "row = df_cut.iloc[4]\n",
    "doc = row['tokens']\n",
    "for start, end, label in row['labels']:\n",
    "    print(start, end, label)\n",
    "print(\"\\n\")\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count and visualise the amount of labels\n",
    "\n",
    "DOC_NAME_COUNT = 0\n",
    "DATE_COUNT = 0\n",
    "PARTIES_COUNT = 0\n",
    "for index, row in df_cut.iterrows():\n",
    "    for l in row['labels']:\n",
    "        if l[2] == \"DOC_NAME\":\n",
    "            DOC_NAME_COUNT += 1\n",
    "        if l[2] == \"AGMT_DATE\":\n",
    "            DATE_COUNT += 1\n",
    "        if l[2] == \"PARTY\":\n",
    "            PARTIES_COUNT += 1\n",
    "\n",
    "# Create DataFrame for the bar plot\n",
    "data=pd.DataFrame.from_dict({\"Document Name\":[DOC_NAME_COUNT],\n",
    "                             \"Date of Agreement\":[DATE_COUNT],\n",
    "                             \"Parties\":[PARTIES_COUNT]})\n",
    "\n",
    "# Use Seaborn for the bar plot\n",
    "splot = sns.barplot(palette=\"pastel\", data=data)\n",
    "splot.set(title='Number of labels in dataset', ylabel='Count')\n",
    "\n",
    "# Annotate the bars with the count of labels\n",
    "for p in splot.patches:\n",
    "    splot.annotate(format(p.get_height(), '.0f'), \n",
    "                   (p.get_x() + p.get_width() / 2.,p.get_height()), \n",
    "                   ha = 'center', va = 'center', \n",
    "                   size=10,\n",
    "                   xytext = (0, -12), \n",
    "                   textcoords = 'offset points')\n",
    "# Show plot\n",
    "plt.show\n",
    "\n",
    "print(\"The total number of labels in the dataset is:\", DOC_NAME_COUNT+DATE_COUNT+PARTIES_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how the entity labels match up with the tokens\n",
    "ents=[]\n",
    "\n",
    "for start, end, label in row['labels']:\n",
    "    if doc.char_span(start, end, label) != None:\n",
    "        ent = doc.char_span(start, end, label)\n",
    "        ents.append(ent)\n",
    "    elif doc.char_span(start, end+1, label) != None:\n",
    "        ent = doc.char_span(start, end+1, label)\n",
    "        ents.append(ent)\n",
    "    elif doc.char_span(start+1, end, label) != None:\n",
    "        ent = doc.char_span(start+1, end, label)\n",
    "        ents.append(ent)\n",
    "    elif doc.char_span(start, end-1, label) != None:\n",
    "        ent = doc.char_span(start, end-1, label)\n",
    "        ents.append(ent)\n",
    "doc.ents = ents\n",
    "doc.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each word must be seperated for the transformer using the IOB format\n",
    "# Create tags using token.ent_iob_ and add to the DataFrame\n",
    "# Allow for any character misalignment between spaCy tokenization and Doccano character indices\n",
    "tags_list_iob = []\n",
    "for index, row in df_cut.iterrows():\n",
    "    doc = row['tokens']\n",
    "    ents=[]\n",
    "    for start, end, label in row['labels']:\n",
    "        if doc.char_span(start, end, label) != None:\n",
    "            ent = doc.char_span(start, end, label)\n",
    "            ents.append(ent)\n",
    "        elif doc.char_span(start, end+1, label) != None:\n",
    "            ent = doc.char_span(start, end+1, label)\n",
    "            ents.append(ent)\n",
    "        elif doc.char_span(start+1, end, label) != None:\n",
    "            ent = doc.char_span(start+1, end, label)\n",
    "            ents.append(ent)\n",
    "        elif doc.char_span(start, end-1, label) != None:\n",
    "            ent = doc.char_span(start, end-1, label)\n",
    "            ents.append(ent)\n",
    "        elif doc.char_span(start-1, end, label) != None:\n",
    "            ent = doc.char_span(start-1, end, label)\n",
    "            ents.append(ent)\n",
    "    doc.ents = ents\n",
    "    iob_tags = [f\"{t.ent_iob_}-{t.ent_type_}\" if t.ent_iob_ != \"O\" else \"O\" for t in doc]\n",
    "    tags_list_iob.append(iob_tags)\n",
    "df_cut['tags'] = tags_list_iob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to ensure we have all the data (all non-null)\n",
    "df_cut.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate list of the IOB feature class labels from tags\n",
    "all_tags = list(itertools.chain.from_iterable(tags_list_iob))\n",
    "\n",
    "def unique(list1):\n",
    "    # insert the list to the set\n",
    "    list_set = set(list1)\n",
    "    # convert the set to the list\n",
    "    unique_list = (list(list_set))\n",
    "    unique_list.sort()\n",
    "    return unique_list\n",
    "\n",
    "feature_class_labels = unique(all_tags)\n",
    "print(feature_class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the NER index tags for each token\n",
    "df_cut['ner_tags'] = df_cut['tags'].apply(lambda x: [feature_class_labels.index(tag) for tag in x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split tokens into a list ready for CSV\n",
    "df_cut['split_tokens'] = df_cut['tokens'].apply(lambda x: [tok.text for tok in x])\n",
    "\n",
    "# Check dataframe head\n",
    "df_cut.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export relevant columns only:\n",
    "export_columns = ['id', 'ner_tags', 'split_tokens']\n",
    "export_df = df_cut[export_columns]\n",
    "export_df.to_json(DATA_FILE, orient=\"table\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export Feature Class Labels for use in Transformer fine tuning\n",
    "with open(FEATURE_CLASS_LABELS, 'w') as f:\n",
    "    json.dump(feature_class_labels, f, indent=2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The dataset is now ready for any transformer model!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
