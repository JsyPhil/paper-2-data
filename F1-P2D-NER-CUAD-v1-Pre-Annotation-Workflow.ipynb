{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paper 2 Data Workflow for Data Extraction - CUADv1 - Pre-Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the various libraries\n",
    "import re, json, os, itertools\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. File handling - CUADv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to each individual txt files converted from PDF\n",
    "TC_PATH = \"../CUAD-v1/full_contract_txt/\"\n",
    "\n",
    "# Path to folder containing all the CUAD data and files\n",
    "MASTER_PATH = \"../CUAD-v1/\"\n",
    "\n",
    "# Name of CSV file containing all the extracted clauses from the Atticus team\n",
    "MASTER_CLAUSES = 'master_clauses.csv'\n",
    "\n",
    "# Name of JSON file to export the agreement text and labels for data extraction\n",
    "JSON_EXPORT = 'jsonl_cuadv1.json'\n",
    "\n",
    "# Name of JSON file to export the agreement taxt and labels for further inspection\n",
    "JSON_EXPORT_INSPECT = 'jsonl_cuadv1_inspect.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Text Data Preprocessing - CUADv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text Files</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2ThemartComInc_19990826_10-12G_EX-10.10_670028...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABILITYINC_06_15_2020-EX-4.25-SERVICES AGREEME...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ACCELERATEDTECHNOLOGIESHOLDINGCORP_04_24_2003-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ACCURAYINC_09_01_2010-EX-10.31-DISTRIBUTOR AGR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ADAMSGOLFINC_03_21_2005-EX-10.17-ENDORSEMENT A...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Text Files\n",
       "0  2ThemartComInc_19990826_10-12G_EX-10.10_670028...\n",
       "1  ABILITYINC_06_15_2020-EX-4.25-SERVICES AGREEME...\n",
       "2  ACCELERATEDTECHNOLOGIESHOLDINGCORP_04_24_2003-...\n",
       "3  ACCURAYINC_09_01_2010-EX-10.31-DISTRIBUTOR AGR...\n",
       "4  ADAMSGOLFINC_03_21_2005-EX-10.17-ENDORSEMENT A..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Walk through all .txt filenames and create a dataframe with the names of the files, sorted alpha/num\n",
    "text_files = []\n",
    "for (dirpath, dirnames, filenames) in os.walk(TC_PATH):\n",
    "    text_files.extend(filenames)\n",
    "\n",
    "tf_df = pd.DataFrame(data = text_files, columns = ['Text Files'])\n",
    "tf_df.sort_values('Text Files', axis=0, inplace=True, ignore_index=True) \n",
    "tf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read master clauses CSV into a dataframe, sort by filename to match text file dataframe created above\n",
    "mc_df = pd.read_csv(MASTER_PATH+MASTER_CLAUSES)\n",
    "\n",
    "# Cut out the relevant info\n",
    "mc_df_cut = mc_df[['Filename',\n",
    "                   'Document Name',\n",
    "                   'Document Name-Answer',\n",
    "                   'Parties',\n",
    "                   'Parties-Answer',\n",
    "                   'Agreement Date',\n",
    "                   'Agreement Date-Answer']].copy()\n",
    "\n",
    "# Sort the dataframe by filename\n",
    "mc_df_cut.sort_values('Filename', axis=0, inplace=True, ignore_index=True) \n",
    "\n",
    "# Bring in the list of the .txt filenames\n",
    "mc_df_cut.insert(loc=1, column='Text Files', value=tf_df)\n",
    "\n",
    "# Create a list of the names of the files, with index num\n",
    "file_list = [(index, row['Text Files']) for index, row in mc_df_cut.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 510 entries, 0 to 509\n",
      "Data columns (total 8 columns):\n",
      " #   Column                 Non-Null Count  Dtype \n",
      "---  ------                 --------------  ----- \n",
      " 0   Filename               510 non-null    object\n",
      " 1   Text Files             510 non-null    object\n",
      " 2   Document Name          510 non-null    object\n",
      " 3   Document Name-Answer   510 non-null    object\n",
      " 4   Parties                510 non-null    object\n",
      " 5   Parties-Answer         509 non-null    object\n",
      " 6   Agreement Date         510 non-null    object\n",
      " 7   Agreement Date-Answer  465 non-null    object\n",
      "dtypes: object(8)\n",
      "memory usage: 32.0+ KB\n"
     ]
    }
   ],
   "source": [
    "#Initial dataframe info\n",
    "mc_df_cut.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to clean up and pre-process the text.\n",
    "# This process should be used for any document text inc. train, validation and test sets.\n",
    "def pre_process_doc_common(text):\n",
    "    # Simple replacement for \"\\n\"\n",
    "    text = text.replace(\"\\n\", \" \")     \n",
    "    \n",
    "    # Simple replacement for \"\\xa0\"\n",
    "    text = text.replace(\"\\xa0\", \" \")  \n",
    "    \n",
    "    # Simple replacement for \"\\x0c\"\n",
    "    text = text.replace(\"\\x0c\", \" \")\n",
    "    \n",
    "    # Get rid of multiple dots\n",
    "    regex = \"\\ \\.\\ \"\n",
    "    subst = \".\"\n",
    "    text = re.sub(regex, subst, text, 0)\n",
    "    \n",
    "    # Get rid of underscores\n",
    "    regex = \"_\"\n",
    "    subst = \" \"\n",
    "    text = re.sub(regex, subst, text, 0)\n",
    "    \n",
    "    # Get rid of multiple dashes\n",
    "    regex = \"--+\"\n",
    "    subst = \" \"\n",
    "    text = re.sub(regex, subst, text, 0)\n",
    "    \n",
    "    # Get rid of multiple stars\n",
    "    regex = \"\\*+\"\n",
    "    subst = \"*\"\n",
    "    text = re.sub(regex, subst, text, 0)\n",
    "    \n",
    "    # Get rid of multiple whitespace\n",
    "    regex = \"\\ +\"\n",
    "    subst = \" \"\n",
    "    text = re.sub(regex, subst, text, 0)\n",
    "    \n",
    "    #Strip leading and trailing whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Function to take in the file list, read each file, clean the text and return all agreements in a list\n",
    "def text_data(file_list, print_text=False, clean_text=True, max_len=3000):\n",
    "    text_list = []\n",
    "    for index, filename in tqdm(file_list):\n",
    "        agreement = open(TC_PATH+filename, \"r\")\n",
    "        text = agreement.read()\n",
    "        if print_text:\n",
    "            print(\"Text before cleaning: \\n\", text)\n",
    "        \n",
    "        # Run text through cleansing function\n",
    "        if clean_text:\n",
    "            text = pre_process_doc_common(text)\n",
    "        text = text[:max_len]\n",
    "        len_text = len(text)\n",
    "        \n",
    "        if print_text:\n",
    "            print(\"Text after cleaning: \\n\", text)\n",
    "        \n",
    "        text_list.append([index,\n",
    "                  filename,\n",
    "                  text,\n",
    "                  len_text])\n",
    "        \n",
    "    return text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 510/510 [00:01<00:00, 405.51it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>Text Files</th>\n",
       "      <th>Document Name</th>\n",
       "      <th>Document Name-Answer</th>\n",
       "      <th>Parties</th>\n",
       "      <th>Parties-Answer</th>\n",
       "      <th>Agreement Date</th>\n",
       "      <th>Agreement Date-Answer</th>\n",
       "      <th>Text</th>\n",
       "      <th>Length_Of_Text</th>\n",
       "      <th>Doc_N_Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2ThemartComInc_19990826_10-12G_EX-10.10_670028...</td>\n",
       "      <td>2ThemartComInc_19990826_10-12G_EX-10.10_670028...</td>\n",
       "      <td>[CO-BRANDING AND ADVERTISING AGREEMENT]</td>\n",
       "      <td>CO-BRANDING AND ADVERTISING AGREEMENT</td>\n",
       "      <td>[2THEMART.COM, INC., 2TheMart, i-Escrow, I-ESC...</td>\n",
       "      <td>I-ESCROW, INC.  (\"i-Escrow\" ); 2THEMART.COM, I...</td>\n",
       "      <td>[June 21, 1999]</td>\n",
       "      <td>6/21/99</td>\n",
       "      <td>CO-BRANDING AND ADVERTISING AGREEMENT THIS CO-...</td>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABILITYINC_06_15_2020-EX-4.25-SERVICES AGREEME...</td>\n",
       "      <td>ABILITYINC_06_15_2020-EX-4.25-SERVICES AGREEME...</td>\n",
       "      <td>[Services Agreement]</td>\n",
       "      <td>Services Agreement</td>\n",
       "      <td>[\"Provider\", TELCOSTAR PTE, LTD., Each of the ...</td>\n",
       "      <td>[ * * * ] (\"Provider\"); TELCOSTAR PTE, LTD.; A...</td>\n",
       "      <td>[October 1, 2019]</td>\n",
       "      <td>10/1/19</td>\n",
       "      <td>EXHIBIT 4.25 INFORMATION IN THIS EXHIBIT IDENT...</td>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ACCELERATEDTECHNOLOGIESHOLDINGCORP_04_24_2003-...</td>\n",
       "      <td>ACCELERATEDTECHNOLOGIESHOLDINGCORP_04_24_2003-...</td>\n",
       "      <td>[JOINT VENTURE AGREEMENT]</td>\n",
       "      <td>JOINT VENTURE AGREEMENT</td>\n",
       "      <td>[Pivotal Self Service Tech, Inc., (the \"Partie...</td>\n",
       "      <td>Collectible Concepts Group, Inc. (\"CCGI\"); Piv...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EXHIBIT 10.13 JOINT VENTURE AGREEMENT Collecti...</td>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Filename  \\\n",
       "0  2ThemartComInc_19990826_10-12G_EX-10.10_670028...   \n",
       "1  ABILITYINC_06_15_2020-EX-4.25-SERVICES AGREEME...   \n",
       "2  ACCELERATEDTECHNOLOGIESHOLDINGCORP_04_24_2003-...   \n",
       "\n",
       "                                          Text Files  \\\n",
       "0  2ThemartComInc_19990826_10-12G_EX-10.10_670028...   \n",
       "1  ABILITYINC_06_15_2020-EX-4.25-SERVICES AGREEME...   \n",
       "2  ACCELERATEDTECHNOLOGIESHOLDINGCORP_04_24_2003-...   \n",
       "\n",
       "                             Document Name  \\\n",
       "0  [CO-BRANDING AND ADVERTISING AGREEMENT]   \n",
       "1                     [Services Agreement]   \n",
       "2                [JOINT VENTURE AGREEMENT]   \n",
       "\n",
       "                    Document Name-Answer  \\\n",
       "0  CO-BRANDING AND ADVERTISING AGREEMENT   \n",
       "1                     Services Agreement   \n",
       "2                JOINT VENTURE AGREEMENT   \n",
       "\n",
       "                                             Parties  \\\n",
       "0  [2THEMART.COM, INC., 2TheMart, i-Escrow, I-ESC...   \n",
       "1  [\"Provider\", TELCOSTAR PTE, LTD., Each of the ...   \n",
       "2  [Pivotal Self Service Tech, Inc., (the \"Partie...   \n",
       "\n",
       "                                      Parties-Answer     Agreement Date  \\\n",
       "0  I-ESCROW, INC.  (\"i-Escrow\" ); 2THEMART.COM, I...    [June 21, 1999]   \n",
       "1  [ * * * ] (\"Provider\"); TELCOSTAR PTE, LTD.; A...  [October 1, 2019]   \n",
       "2  Collectible Concepts Group, Inc. (\"CCGI\"); Piv...                 []   \n",
       "\n",
       "  Agreement Date-Answer                                               Text  \\\n",
       "0               6/21/99  CO-BRANDING AND ADVERTISING AGREEMENT THIS CO-...   \n",
       "1               10/1/19  EXHIBIT 4.25 INFORMATION IN THIS EXHIBIT IDENT...   \n",
       "2                   NaN  EXHIBIT 10.13 JOINT VENTURE AGREEMENT Collecti...   \n",
       "\n",
       "   Length_Of_Text  Doc_N_Length  \n",
       "0            1000             1  \n",
       "1            1000             1  \n",
       "2            1000             1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean text and create dataframe with the text of ech document\n",
    "data = text_data(file_list, print_text=False, clean_text=True, max_len=1000)\n",
    "columns = ['ID', 'Documents', 'Text', 'Length_Of_Text']\n",
    "text_df = pd.DataFrame(data=data, columns=columns)\n",
    "\n",
    "# Add the two columns to a copy of the main dataframe\n",
    "mc_df_wk = mc_df_cut.copy()\n",
    "mc_df_wk = mc_df_wk.join(text_df[['Text', 'Length_Of_Text']])\n",
    "\n",
    "#Ensure agreement date, doc_name and parties are list objects\n",
    "mc_df_wk[\"Agreement Date\"] = mc_df_wk[\"Agreement Date\"].apply(eval)\n",
    "mc_df_wk[\"Document Name\"] = mc_df_wk[\"Document Name\"].apply(eval)\n",
    "mc_df_wk[\"Parties\"] = mc_df_wk[\"Parties\"].apply(eval)\n",
    "\n",
    "# Some document name references have more than one entry - remove them for further inspection later\n",
    "mc_df_wk['Doc_N_Length'] = mc_df_wk['Document Name'].str.len()\n",
    "mc_df_mul = mc_df_wk[mc_df_wk.Doc_N_Length > 1]\n",
    "mc_df_wk.drop(mc_df_mul.index, inplace=True)\n",
    "\n",
    "# Have a look at the data\n",
    "mc_df_wk.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 499 entries, 0 to 509\n",
      "Data columns (total 11 columns):\n",
      " #   Column                 Non-Null Count  Dtype \n",
      "---  ------                 --------------  ----- \n",
      " 0   Filename               499 non-null    object\n",
      " 1   Text Files             499 non-null    object\n",
      " 2   Document Name          499 non-null    object\n",
      " 3   Document Name-Answer   499 non-null    object\n",
      " 4   Parties                499 non-null    object\n",
      " 5   Parties-Answer         498 non-null    object\n",
      " 6   Agreement Date         499 non-null    object\n",
      " 7   Agreement Date-Answer  458 non-null    object\n",
      " 8   Text                   499 non-null    object\n",
      " 9   Length_Of_Text         499 non-null    int64 \n",
      " 10  Doc_N_Length           499 non-null    int64 \n",
      "dtypes: int64(2), object(9)\n",
      "memory usage: 46.8+ KB\n"
     ]
    }
   ],
   "source": [
    "# Check for null values\n",
    "mc_df_wk.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 458 entries, 0 to 509\n",
      "Data columns (total 11 columns):\n",
      " #   Column                 Non-Null Count  Dtype \n",
      "---  ------                 --------------  ----- \n",
      " 0   Filename               458 non-null    object\n",
      " 1   Text Files             458 non-null    object\n",
      " 2   Document Name          458 non-null    object\n",
      " 3   Document Name-Answer   458 non-null    object\n",
      " 4   Parties                458 non-null    object\n",
      " 5   Parties-Answer         458 non-null    object\n",
      " 6   Agreement Date         458 non-null    object\n",
      " 7   Agreement Date-Answer  458 non-null    object\n",
      " 8   Text                   458 non-null    object\n",
      " 9   Length_Of_Text         458 non-null    int64 \n",
      " 10  Doc_N_Length           458 non-null    int64 \n",
      "dtypes: int64(2), object(9)\n",
      "memory usage: 42.9+ KB\n"
     ]
    }
   ],
   "source": [
    "# Agreement date is an important label. Here we will drop any agreement without a date.\n",
    "# These will typically be template or specimen agreements which havent been executed\n",
    "# Prior to dropping, we create a dataframe to manually check and annotate agreement date in a different exercise\n",
    "mc_df_nul = mc_df_wk[mc_df_wk[\"Agreement Date-Answer\"].isnull()]\n",
    "mc_df_wk = mc_df_wk.dropna(subset=['Agreement Date-Answer'])\n",
    "mc_df_wk.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The CUADv1 labels includes the Party definition eg Apple Inc. \"Apple\", here we keep just the legal entity:\n",
    "def remove_party_overlaps(labels):\n",
    "    labels.sort()\n",
    "    k = []\n",
    "    for i in range(len(labels)-1):\n",
    "        l1 = labels[i]\n",
    "        l2 = labels[i+1]\n",
    "        if l1[0] == l2[0]:\n",
    "            len1 = l1[1] - l1[0]\n",
    "            len2 = l2[1] - l2[0]\n",
    "            if len1 > len2:\n",
    "                k.append(l1)\n",
    "                continue\n",
    "            else:\n",
    "                k.append(l2)\n",
    "                continue\n",
    "        else:\n",
    "            k.append(labels[i])\n",
    "    new_labels = list(k for k,_ in itertools.groupby(k))\n",
    "    \n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "458it [00:00, 1857.30it/s]\n",
      "11it [00:00, 9427.33it/s]\n",
      "41it [00:00, 13409.74it/s]\n"
     ]
    }
   ],
   "source": [
    "# Go through each label and find the label in the text, ensure label is pre-processed same as text.\n",
    "# If labels don't match, append to a seperate file to check.\n",
    "\n",
    "clean_text = True\n",
    "djson = list()\n",
    "djson_inspect = list()\n",
    "for index, row in tqdm(mc_df_wk.iterrows()):\n",
    "    labels = list()\n",
    "    ids = index\n",
    "    text = row['Text']\n",
    "    \n",
    "    #DOC_NAME\n",
    "    doc_names = row['Document Name']\n",
    "    for name in doc_names:\n",
    "        if clean_text:\n",
    "            name = pre_process_doc_common(name)\n",
    "        matches = re.finditer(re.escape(name.lower()), text.lower())\n",
    "        for m in matches:\n",
    "            s = m.start()\n",
    "            e = m.end()\n",
    "            labels.append([s, e, 'DOC_NAME'])\n",
    "    \n",
    "    #AGMT_DATE\n",
    "    agmt_date = row['Agreement Date']\n",
    "    for date in agmt_date:\n",
    "        if clean_text:\n",
    "            date = pre_process_doc_common(date)\n",
    "        matches = re.finditer(re.escape(date.lower()), text.lower())\n",
    "        for m in matches:\n",
    "            s = m.start()\n",
    "            e = m.end()\n",
    "            labels.append([s, e, 'AGMT_DATE'])\n",
    "\n",
    "    #PARTIES\n",
    "    parties = row['Parties']\n",
    "    for party in parties:\n",
    "        if clean_text:\n",
    "            party = pre_process_doc_common(party)\n",
    "        matches = re.finditer(re.escape(party.lower()), text.lower())\n",
    "        for m in matches:\n",
    "            s = m.start()\n",
    "            e = m.end()\n",
    "            labels.append([s, e, 'PARTY'])\n",
    "    \n",
    "    labels = remove_party_overlaps(labels)\n",
    "    #print(labels)\n",
    "    \n",
    "    # Check for incongruous finds, add to inspect file\n",
    "    flat_list = [item for sublist in labels for item in sublist]\n",
    "    if 'DOC_NAME' in flat_list and 'AGMT_DATE' in flat_list and 'PARTY' in flat_list:\n",
    "        djson.append({'id': ids, 'text': text, \"labels\": labels})\n",
    "    else:\n",
    "        djson_inspect.append({'id': ids, 'text': text, \"labels\": labels})\n",
    "\n",
    "# Add to the check JSON file the other documents excluded due to duplicate names and no agreement dates\n",
    "for index, row in tqdm(mc_df_mul.iterrows()):\n",
    "    labels = list()\n",
    "    ids = index\n",
    "    text = row['Text']\n",
    "    djson_inspect.append({'id': ids, 'text': text, \"labels\": labels})\n",
    "\n",
    "for index, row in tqdm(mc_df_nul.iterrows()):\n",
    "    labels = list()\n",
    "    ids = index\n",
    "    text = row['Text']\n",
    "    djson_inspect.append({'id': ids, 'text': text, \"labels\": labels})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are left with 349 training samples out of 510 to annotate.\n",
      "Additional agreements to check:  161\n"
     ]
    }
   ],
   "source": [
    "# The process above requires the three label types to be present in each agreement extract. This may not\n",
    "# be the case due to the shortening of the agreememt for example. Let's check how many we are left with\n",
    "# and how many we need to manually check...\n",
    "print(f\"We are left with {len(djson)} training samples out of 510 to annotate.\")\n",
    "print(\"Additional agreements to check: \",len(djson_inspect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Check for erroneous labels\n",
    "count = 0\n",
    "for n in range(len(djson)):\n",
    "    labs = djson[n]['labels']\n",
    "    flat_list = [item for sublist in labs for item in sublist]\n",
    "    if -1 in flat_list:\n",
    "        count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "187442"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Export the full datasets for import to Doccano\n",
    "filepath = JSON_EXPORT\n",
    "open(filepath, 'w').write(\"\\n\".join([json.dumps(e) for e in djson]))\n",
    "\n",
    "filepath = JSON_EXPORT_INSPECT\n",
    "open(filepath, 'w').write(\"\\n\".join([json.dumps(e) for e in djson_inspect]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Doccano to tag the text file dataset:\n",
    " - Install doccano at the command line: pip install doccano\n",
    " - At the command line change the directory to this directory\n",
    " - run doccano at the command line by typing 'doccano'\n",
    " - Application will be running at http://0.0.0.0:8000/\n",
    " - Username is 'admin', passowrd is 'password'\n",
    " - Use ctrl-c to end application"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
